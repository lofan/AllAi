<a href="https://github.com/Hannibal046/Awesome-LLM">Awesome LLM
<p></p>
<h2>開源大語言模型完整清單</h2>
<p></p>
Large Language Model (LLM) 即大規模語言模型，是一種基於深度學習的自然語言處理模型，它能夠學習到自然語言的語法和語義，從而可以生成人類可讀的文本。
所謂"語言模型"，就是只用來處理語言文字（或者符號體系）的 AI 模型，發現其中的規律，可以根據提示 (prompt)，自動生成符合這些規律的內容。
LLM 通常基於神經網路模型，使用大規模的語料庫進行訓練，比如使用互聯網上的海量文本資料。這些模型通常擁有數十億到數萬億個參數，能夠處理各種自然語言處理任務，如自然語言生成、文本分類、文本摘要、機器翻譯、語音辨識等。
本文對國內外公司、科研機構等組織開源的 LLM 進行了全面的整理。
<p></p>
<b>開源中文 LLM</b>
<a href="https://github.com/THUDM/ChatGLM-6B">ChatGLM-6B —— 雙語對話語言模型</a>
ChatGLM-6B 是一個開源的、支持中英雙語問答的對話語言模型，並針對中文進行了優化。該模型基於 General Language Model (GLM) 架構，具有 62 億參數。結合模型量化技術，使用者可以在消費級的顯卡上進行本地部署（INT4 量化級別下最低只需 6GB 顯存）。
ChatGLM-6B 使用了和 ChatGLM 相同的技術，針對中文問答和對話進行了優化。經過約 1T 識別字的中英雙語訓練，輔以監督微調、回饋自助、人類回饋強化學習等技術的加持，62 億參數的 ChatGLM-6B 雖然規模不及千億模型，但大大降低了推理成本，提升了效率，並且已經能生成相當符合人類偏好的回答。
<p></p>
<a href="https://github.com/THUDM/ChatGLM2-6B">ChatGLM2-6B —— 中英雙語對話模型 ChatGLM-6B 的第二代版本</a>
基於 ChatGLM 初代模型的開發經驗，ChatGLM2-6B 全面升級了基座模型，更長的上下文，更高效的推理、更開放的協定。
<p></p>
<a href="https://github.com/THUDM/VisualGLM-6B">VisualGLM-6B —— 多模態對話語言模型</a>a>
VisualGLM-6B 是一個開源的，支援圖像、中文和英文的多模態對話語言模型，語言模型基於 ChatGLM-6B，具有 62 億參數；圖像部分通過訓練 BLIP2-Qformer 構建起視覺模型與語言模型的橋樑，整體模型共78億參數。
<p></p>
<a href="https://www.oschina.net/p/moss">MOSS —— 支援中英雙語的對話大語言模型</a>a>
MOSS 是一個支持中英雙語和多種外掛程式的開源對話語言模型， moss-moon 系列模型具有 160 億參數，在 FP16 精度下可在單張 A100/A800 或兩張 3090 顯卡運行，在 INT4/8 精度下可在單張 3090 顯卡運行。
MOSS 基座語言模型在約七千億中英文以及代碼單詞上預訓練得到，後續經過對話指令微調、外掛程式增強學習和人類偏好訓練具備多輪對話能力及使用多種外掛程式的能力。
<p></p>
<a href="https://www.oschina.net/p/db-gpt">DB-GPT —— 資料庫大語言模型</a>a>
DB-GPT 是一個開源的以資料庫為基礎的 GPT 實驗項目，使用當地語系化的 GPT 大模型與資料和環境進行交互，無數據洩露風險，100% 私密，100% 安全。
DB-GPT 為所有以資料庫為基礎的場景，構建了一套完整的私有大模型解決方案。 此方案因為支持本地部署，所以不僅僅可以應用於獨立私有環境，而且還可以根據業務模組獨立部署隔離，讓大模型的能力絕對私有、安全、可控。
<p></p>
CPM-Bee —— 中英文雙語大語言模型
CPM-Bee 是一個 完全開源、允許商用的百億參數中英文基座模型。它採用 Transformer 自回歸架構（auto-regressive），使用萬億級高品質語料進行預訓練，擁有強大的基礎能力。
CPM-Bee 的特點可以總結如下：
•	開源可商用：OpenBMB 始終秉承 “讓大模型飛入千家萬戶” 的開源精神，CPM-Bee 基座模型將完全開源並且可商用，以推動大模型領域的發展。如需將模型用於商業用途，只需企業實名郵件申請並獲得官方授權證書，即可商用使用。
•	中英雙語性能優異：CPM-Bee 基座模型在預訓練語料上進行了嚴格的篩選和配比，同時在中英雙語上具有亮眼表現，具體可參見評測任務和結果。
•	超大規模高品質語料：CPM-Bee 基座模型在萬億級語料上進行訓練，是開源社區內經過語料最多的模型之一。同時，我們對預訓練語料進行了嚴格的篩選、清洗和後處理以確保品質。
•	OpenBMB 大模型系統生態支援：OpenBMB 大模型系統在高性能預訓練、適配、壓縮、部署、工具開發了一系列工具，CPM-Bee 基座模型將配套所有的工具腳本，高效支持開發者進行進階使用。
•	強大的對話和工具使用能力：結合 OpenBMB 在指令微調和工具學習的探索，我們在 CPM-Bee 基座模型的基礎上進行微調，訓練出了具有強大對話和工具使用能力的實例模型，現已開放定向邀請內測，未來會逐步向公眾開放。
CPM-Bee 的基座模型可以準確地進行語義理解，高效完成各類基礎任務，包括：文字填空、文本生成、翻譯、問答、評分預測、文本選擇題等等。
<p></p>
LaWGPT —— 基於中文法律知識的大語言模型
LaWGPT 是一系列基於中文法律知識的開源大語言模型。
該系列模型在通用中文基座模型（如 Chinese-LLaMA、ChatGLM 等）的基礎上擴充法律領域專有詞表、大規模中文法律語料預訓練，增強了大模型在法律領域的基礎語義理解能力。在此基礎上，構造法律領域對話問答資料集、中國司法考試資料集進行指令精調，提升了模型對法律內容的理解和執行能力。
<p></p>
伶荔 (Linly) —— 大規模中文語言模型
相比已有的中文開源模型，伶荔模型具有以下優勢：
1.	在 32*A100 GPU 上訓練了不同量級和功能的中文模型，對模型充分訓練並提供強大的 baseline。據知，33B 的 Linly-Chinese-LLAMA 是目前最大的中文 LLaMA 模型。
2.	公開所有訓練資料、代碼、參數細節以及實驗結果，確保項目的可複現性，用戶可以選擇合適的資源直接用於自己的流程中。
3.	項目具有高相容性和易用性，提供可用於 CUDA 和 CPU 的量化推理框架，並支持 Huggingface 格式。
目前公開可用的模型有：
•	Linly-Chinese-LLaMA：中文基礎模型，基於 LLaMA 在高品質中文語料上增量訓練強化中文語言能力，現已開放 7B、13B 和 33B 量級，65B 正在訓練中。
•	Linly-ChatFlow：中文對話模型，在 400 萬指令資料集合上對中文基礎模型指令精調，現已開放 7B、13B 對話模型。
•	Linly-ChatFlow-int4 ：ChatFlow 4-bit 量化版本，用於在 CPU 上部署模型推理。
進行中的項目：
•	Linly-Chinese-BLOOM：基於 BLOOM 中文增量訓練的中文基礎模型，包含 7B 和 175B 模型量級，可用於商業場景。
<p></p>
Chinese-Vicuna —— 基於 LLaMA 的中文大語言模型
Chinese-Vicuna 是一個中文低資源的 LLaMA+Lora 方案。
項目包括
•	finetune 模型的代碼
•	推理的代碼
•	僅使用 CPU 推理的代碼 (使用 C++)
•	下載 / 轉換 / 量化 Facebook llama.ckpt 的工具
•	其他應用
<p></p>
Chinese-LLaMA-Alpaca —— 中文 LLaMA & Alpaca 大模型
Chinese-LLaMA-Alpaca 包含中文 LLaMA 模型和經過指令微調的 Alpaca 大型模型。
這些模型在原始 LLaMA 的基礎上，擴展了中文詞彙表並使用中文資料進行二次預訓練，從而進一步提高了對中文基本語義理解的能力。同時，中文 Alpaca 模型還進一步利用中文指令資料進行微調，明顯提高了模型對指令理解和執行的能力。
<p></p>
ChatYuan —— 對話語言大模型
ChatYuan 是一個支持中英雙語的功能型對話語言大模型。ChatYuan-large-v2 使用了和 v1 版本相同的技術方案，在微調資料、人類回饋強化學習、思維鏈等方面進行了優化。
ChatYuan-large-v2 是 ChatYuan 系列中以輕量化實現高品質效果的模型之一，使用者可以在消費級顯卡、 PC 甚至手機上進行推理（INT4 最低只需 400M ）。
<p></p>
華佗 GPT —— 開源中文醫療大模型
HuatuoGPT（華佗 GPT）是開源中文醫療大模型，基於醫生回復和 ChatGPT 回復，讓語言模型成為醫生，提供豐富且準確的問診。
HuatuoGPT 致力於通過融合 ChatGPT 生成的 “蒸餾資料” 和真實世界醫生回復的資料，以使語言模型具備像醫生一樣的診斷能力和提供有用資訊的能力，同時保持對使用者流暢的交互和內容的豐富性，對話更加絲滑。
<p></p>
本草 —— 基於中文醫學知識的 LLaMA 微調模型
本草(BenTsao)【原名：華駝 (HuaTuo)】是基於中文醫學知識的 LLaMA 微調模型。
此專案開源了經過中文醫學指令精調 / 指令微調 (Instruct-tuning) 的 LLaMA-7B 模型。通過醫學知識圖譜和 GPT3.5 API 構建了中文醫學指令資料集，並在此基礎上對 LLaMA 進行了指令微調，提高了 LLaMA 在醫療領域的問答效果。
<p></p>
鵬程·盤古α —— 中文預訓練語言模型
「鵬程·盤古α」是業界首個 2000 億參數以中文為核心的預訓練生成語言模型，目前開源了兩個版本：鵬程·盤古α和鵬程·盤古α增強版，並支持NPU和GPU兩個版本，支援豐富的場景應用，在知識問答、知識檢索、知識推理、閱讀理解等文本生成領域表現突出，具備較強的少樣本學習的能力。
基於盤古系列大模型提供大模型應用落地技術説明使用者高效的落地超大預訓練模型到實際場景。整個框架特點如下：
主要有如下幾個核心模組：
•	資料集：從開源開放資料集、common crawl 資料集、電子書等收集近 80TB 原始語料，構建了約 1.1TB 的高品質中文語料資料集、53 種語種高品質單、雙語資料集 2TB。
•	基礎模組：提供預訓練模型庫，支援常用的中文預訓練模型，包括鵬程・盤古 α、鵬程・盤古 α 增強版等。
•	應用層：支援常見的 NLP 應用比如多語言翻譯、開放域對話等，支援預訓練模型落地工具，包括模型壓縮、框架移植、可持續學習，助力大模型快速落地。
鵬程·盤古對話生成大模型
鵬程・盤古對話生成大模型 (PanGu-Dialog)。
PanGu-Dialog 是以大資料和大模型為顯著特徵的大規模開放域對話生成模型，充分利用大規模預訓練語言模型的知識和語言能力，構建可控、可靠可信、有智慧的自然人機對話模型。主要特性如下：
•	首次提出對話智慧度以探索對話模型的邏輯推理、資料計算、聯想、創作等方面的能力。
•	構建了覆蓋領域最廣 (據我們所知) 的開放域互動式對話評估資料集 PGCED，12 個領域，並在知識性、安全性、智慧程度等方面製作了針對性的評測資料。
•	基於預訓練 + 持續微調的學習策略融合大規模普通文本和多種對話資料訓練而成，充分利用訓練語言模型語言能力和知識，高效構建強大的對話模型。
•	在各項指標上達到了中文純模型生成式對話 SOTA 水準，在知識性和信息量方面優勢明顯，但安全性、可靠、可信、可控、智慧等方面的提升並不明顯。
•	目前生成式對話仍處於較低水準，與人類對話能力存在明顯的差距，後續將在現有基礎上針對不同的維度不斷優化反覆運算，不斷進步。
<p></p>
悟道 —— 雙語多模態大語言模型
“悟道” 是雙語多模態預訓練模型，規模達到 1.75 萬億參數。專案現有 7 個開源模型成果。
<p></p>
圖文類
•	CogView
CogView 參數量為 40 億，模型可實現文本生成圖像，經過微調後可實現國畫、油畫、水彩畫、輪廓畫等圖像生成。目前在公認 MS COCO 文生圖任務上取得了超過 OpenAI DALL・E 的成績，獲得世界第一。
<p></p>
•	BriVL
BriVL (Bridging Vision and Language Model) 是首個中文通用圖文多模態大規模預訓練模型。BriVL 模型在圖文檢索任務上有著優異的效果，超過了同期其他常見的多模態預訓練模型（例如 UNITER、CLIP）。
<p></p>
文本類
<p></p>
•	GLM
GLM 是以英文為核心的預訓練語言模型系列，基於新的預訓練范式實現單一模型在語言理解和生成任務方面取得了最佳結果，並且超過了在相同資料量進行訓練的常見預訓練模型（例如 BERT，RoBERTa 和 T5），目前已開源 1.1 億、3.35 億、4.10 億、5.15 億、100 億參數規模的模型。
<p></p>
•	CPM
CPM 系列模型是兼顧理解與生成能力的預訓練語言模型系列，涵蓋中文、中英雙語多類模型，目前已開源 26 億、110 億和 1980 億參數規模的模型。
<p></p>
•	Transformer-XL
Transformer-XL 是以中文為核心的預訓練語言生成模型，參數規模為 29 億，目前可支援包括文章生成、智慧作詩、評論 / 摘要生成等主流 NLG 任務。
<p></p>
•	EVA
EVA 是一個開放領域的中文對話預訓練模型，是目前最大的漢語對話模型，參數量達到 28 億，並且在包括不同領域 14 億漢語的悟道對話資料集（WDC）上進行預訓練。
<p></p>
•	Lawformer
Lawformer 是世界首創法律領域長文本中文預訓練模型，參數規模達到 1 億。
<p></p>
蛋白質類
<p></p>
•	ProtTrans
ProtTrans 是國內最大的蛋白質預訓練模型，參數總量達到 30 億。
<p></p>
BBT-2 —— 120 億參數大語言模型
BBT-2 是包含 120 億參數的通用大語言模型，在 BBT-2 的基礎上訓練出了代碼，金融，文生圖等專業模型。基於 BBT-2 的系列模型包括：
•	BBT-2-12B-Text：120 億參數的中文基礎模型
•	BBT-2.5-13B-Text: 130 億參數的中文+英文雙語基礎模型
•	BBT-2-12B-TC-001-SFT 經過指令微調的代碼模型，可以進行對話
•	BBT-2-12B-TF-001 在 120 億模型上訓練的金融模型，用於解決金融領域任務
•	BBT-2-12B-Fig：文生圖模型
•	BBT-2-12B-Science 科學論文模型
<p></p>
BELLE —— 開源中文對話大模型
BELLE: Be Everyone's Large Language model Engine（開源中文對話大模型）
本專案目標是促進中文對話大模型開源社區的發展，願景做能幫到每一個人的 LLM Engine。現階段本專案基於一些開源預訓練大語言模型（如 BLOOM），針對中文做了優化，模型調優僅使用由 ChatGPT 生產的資料（不包含任何其他資料）。
<p></p>
TigerBot —— 多模態大語言模型
TigerBot 是一個多語言多工的大規模語言模型(LLM)。根據 OpenAI InstructGPT 論文在公開 NLP 資料集上的自動評測，TigerBot-7B 達到 OpenAI 同樣大小模型的綜合表現的 96%。
<p></p>
YuLan-Chat —— 大語言對話模型
中國人民大學高瓴人工智慧學院相關研究團隊（由多位學院老師聯合指導）展開了一系列關於指令微調技術的研究，並發佈了學院初版大語言對話模型——YuLan-Chat，旨在探索和提升大語言模型的中英文雙語對話能力。
<p></p>
百聆(BayLing) —— 具有增強的語言對齊的英語/中文大語言模型
“百聆”是中國科學院計算技術研究所自然語言處理團隊開發一個具有增強的語言對齊的英語/中文大語言模型。“百聆”具有優越的英語/中文生成能力、指令遵循能力和多輪交互能力，在多項測試中取得ChatGPT 90%的性能。“百聆”內測已經開啟，歡迎試用。
<p></p>

開源 LLM
<p></p>
通義千問-7B —— 基於 Transformer 的大語言模型
通義千問 - 7B（Qwen-7B） 是阿裡雲研發的通義千問大模型系列的 70 億參數規模的模型。
Qwen-7B 基於 Transformer 在超大規模的預訓練資料上進行訓練得到。預訓練資料類型多樣，覆蓋廣泛，包括大量網路文本、專業書籍、代碼等。同時，在 Qwen-7B 的基礎上，使用對齊機制打造了基於大語言模型的 AI 助手 Qwen-7B-Chat。
Qwen-7B 系列模型的特點包括：
1.	大規模高品質預訓練資料：使用了超過 2.2 萬億 token 的自建大規模預訓練資料集進行語言模型的預訓練。資料集包括文本和代碼等多種資料類型，覆蓋通用領域和專業領域。
2.	優秀的模型性能：相比同規模的開源模型，Qwen-7B 在多個評測資料集上具有顯著優勢，甚至超出 12-13B 等更大規模的模型。評測評估的能力範圍包括自然語言理解與生成、數學運算解題、代碼生成等。
3.	更好地支援多語言：基於更大詞表的分詞器在分詞上更高效，同時它對其他語言表現更加友好。用戶可以在 Qwen-7B 的基礎上更方便地訓練特定語言的 7B 語言模型。
4.	8K 的上下文長度：Qwen-7B 及 Qwen-7B-Chat 均能支援 8K 的上下文長度，允許用戶輸入更長的 prompt。
5.	支援外掛程式調用：Qwen-7B-Chat 針對外掛程式調用相關的對齊資料做了特定優化，當前模型能有效調用外掛程式以及升級為 Agent。
<p></p>
Code Llama —— 基於 Llama 2 的 AI 代碼生成大模型
Code Llama 是基於 Llama 2 的 AI 代碼生成大模型，可根據代碼和自然語言提示生成代碼和有關代碼的自然語言，支援多種主流程式設計語言，包括 Python、C++、Java、PHP、Typescript (Javascript)、C# 和 Bash。
Code Llama 基於 Llama 2 大語言模型打造，提供了三種模型：
•	Code Llama- 基礎代碼模型
•	Code Llama - Python- 專門針對 Python 進行優化
•	Code Llama - Instruct- 專門用於理解自然語言指令
它們具有開放式模型中領先的性能、填充能力、對大型輸入上下文的支援以及用於程式設計任務的零指令跟隨能力。所有模型都是基於 16k 標記序列進行訓練，並在最多 100k 標記輸入上顯示出改進。
<p></p>
CodeFuse-13B —— 代碼大語言模型
CodeFuse-13B 是基於 GPT-NeoX 框架訓練的 13B 參數代碼生成模型，能夠處理 4096 個字元的代碼序列。
該模型在 1000B Token 的代碼、中文、英文資料資料集上進行預訓練，覆蓋超過 40 種程式設計語言。
為了進一步提升生成代碼的效果和品質，該模型還在 CodeFuse-Evol-instruction-66k 資料集上進行了微調，使得該模型能夠生成更加準確、高效、符合要求的代碼。在 HumanEval 評測集上 Pass@1 達到 37.1%(採用 BeamSearch 解碼，其中 BeamSize=3)。
<p></p>
MiLM-6B —— 小米 AI 大模型
MiLM-6B 是由小米開發的一個大規模預訓練語言模型，參數規模為 64 億。在 C-Eval 和 CMMLU 上均取得同尺寸最好的效果。
根據 C-Eval 給出的資訊，MiLM-6B 模型在具體各科目成績上，在 STEM（科學、技術、工程和數學教育）全部 20 個科目中，計量師、物理、化學、生物等多個專案獲得了較高的準確率。
LLaMA —— Meta 大語言模型
LLaMA 語言模型全稱為 "Large Language Model Meta AI"，是 Meta 的全新大型語言模型（LLM），這是一個模型系列，根據參數規模進行了劃分（分為 70 億、130 億、330 億和 650 億參數不等）。
其中 LaMA-13B（130 億參數的模型）儘管模型參數相比 OpenAI 的 GPT-3（1750 億參數） 要少了十幾倍，但在性能上反而可以超過 GPT-3 模型。更小的模型也意味著開發者可以在 PC 甚至是智慧手機等設備上本地運行類 ChatGPT 這樣的 AI 助手，無需依賴資料中心這樣的大規模設施。
Stanford Alpaca —— 指令調優的 LLaMA 模型
Stanford Alpaca（斯坦福 Alpaca）是一個指令調優的 LLaMA 模型，從 Meta 的大語言模型 LLaMA 7B 微調而來。
Stanford Alpaca 讓 OpenAI 的 text-davinci-003 模型以 self-instruct 方式生成 52K 指令遵循（instruction-following）樣本，以此作為 Alpaca 的訓練資料。研究團隊已將訓練資料、生成訓練資料的代碼和超參數開源，後續還將發佈模型權重和訓練代碼。
Lit-LLaMA —— 基於 nanoGPT 的語言模型
Lit-LLaMA 是一個基於 nanoGPT 的 LLaMA 語言模型的實現，支援量化、LoRA 微調、預訓練、flash attention、LLaMA-Adapter 微調、Int8 和 GPTQ 4bit 量化。
主要特點：單一檔實現，沒有樣板代碼；在消費者硬體上或大規模運行；在數值上等同於原始模型。
Lit-LLaMA 認為人工智慧應該完全開源並成為集體知識的一部分。但原始的 LLaMA 代碼採用 GPL 許可證，這意味著使用它的任何專案也必須在 GPL 下發佈。這“污染”了其他代碼，阻止了與生態系統的集成。Lit-LLaMA 永久性地解決了這個問題。
GloVe —— 斯坦福大學的詞向量工具
GloVe的全稱叫Global Vectors for Word Representation，它是一個基於全域詞頻統計（count-based & overall statistics）的詞表征（word representation）工具，它可以把一個單詞表達成一個由實數組成的向量，這些向量捕捉到了單詞之間一些語義特性，比如相似性（similarity）、類比性（analogy）等。我們通過對向量的運算，比如歐幾裡得距離或者cosine相似度，可以計算出兩個單詞之間的語義相似性。
以下是 GloVe 提供的預訓練詞向量，遵循 Public Domain Dedication and License 許可。
•	Wikipedia 2014+Gigaword 5(6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download):glove.6B.zip
•	Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download):glove.42B.300d.zip
•	Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download):glove.840B.300d.zip
•	Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download):glove.twitter.27B.zip
Dolly —— 低成本大語言模型
Dolly 是一個低成本的 LLM，Dolly 採用 EleutherAI 現有的 60 億參數的開源模型，並對其進行細微的修改，以激發指令跟隨能力。
儘管模型小得多，只有 60 億個參數，以及較小的資料集和訓練時間（ChatGPT 的參數是 1750 億個），但 Dolly 仍然表現出了 ChatGPT 所展示的同樣的 "神奇的人類互動能力"。
OPT-175B —— Meta 開源的大語言模型
OPT-175B 是 Meta 開源的大語言模型，擁有超過 1750 億個參數 —— 和 GPT-3 相當。相比 GPT-3，OPT-175B 的優勢在於它完全免費。
Meta 還公佈了代碼庫、開發過程日誌、資料、研究論文和其他與 OPT-175B 相關的資訊。儘管 OPT-175B 是免費的，但 Meta 也給出了一些限制。為了防止誤用和 “保持完整性”，OPT-175B 只允許在非商業用途下使用。也就是說，OPT-175B 的多數應用場景還是在科研上。
Cerebras-GPT —— 自然語言處理領域大模型
Cerebras GPT 是由 Cerebras 公司開源的自然語言處理領域的預訓練大模型，其模型參數規模最小 1.11 億，最大 130 億，共 7 個模型。
與業界的模型相比，Cerebras-GPT 幾乎是各個方面完全公開，沒有任何限制。不管是模型架構，還是預訓練結果都是公開的。
BLOOM —— 自然語言處理大模型
Bloom 是用於自然語言處理的大語言模型，包含 1760 億個參數，支援 46 種自然語言（包括中文）和 13 種程式設計語言，可以用來回答問題、翻譯文本、從檔中提取資訊片段，還能像 GitHub Copilot 一樣用於生成代碼。
BLOOM 模型的最大優勢是它的易獲取性，任何個人或機構都可以從 Hugging Face 免費獲得 1760 億個參數的完整模型。用戶有多個語種可選，然後將需求輸入到 BLOOM 中，任務類型包括撰寫食譜或詩歌、翻譯或總結文本，甚至還有代碼程式設計。人工智慧開發者可以在該模型的基礎上構建他們自己的應用程式。
BLOOMChat —— 176B 的開源可商用多語言聊天 LLM
BLOOMChat 是一個新的、開放的、多語言的聊天 LLM。SambaNova 和 Together 使用 SambaNova 獨特的可重構資料流程架構在 SambaNova DataScale 系統上訓練了 BLOOMChat；其建立在 BigScience 組織的 BLOOM 之上，並在 OpenChatKit、Dolly 2.0 和 OASST1 的 OIG 上進行了微調。
GPT-J —— 自然語言處理 AI 模型
GPT-J 是一個基於 GPT-3，由 60 億個參數組成的自然語言處理 AI 模型。
該模型在一個 800GB 的開源文本資料集上進行訓練，並且能夠與類似規模的 GPT-3 模型相媲美。 該模型通過利用 Google Cloud 的 v3-256 TPU 以及 EleutherAI 的 The Pile 資料集進行訓練，歷時大約五周時間。GPT-J 在標準 NLP 基準工作負載上實現了與 OpenAI 報告的 67 億參數版本的 GPT-3 類似的準確性。模型代碼、預訓練的權重檔、Colab 文檔和一個演示網頁都包含在 EleutherAI 的開源項目中。
GPT-2 —— 基於 Transformer 的大型語言模型
GPT-2 是一種基於 transformer 的大型語言模型，具有 15 億個參數，在 800 萬網頁資料集上進行訓練。
GPT-2 能夠翻譯文本、回答問題、總結段落，並生成文本輸出。雖然其輸出內容有時與人類相似，但在生成長段落時輸出內容可能會變得重複或無意義。
GPT-2 是一個通用學習器，沒有經過專門訓練來執行任何特定的任務，並且是作為 OpenAI 2018 GPT 模型的“直接擴展”而創建的，其參數數量和訓練資料集的大小均增加了十倍。
RWKV-LM —— 線性 Transformer 模型
RWKV 是結合了 RNN 和 Transformer 的語言模型，適合長文本，運行速度較快，擬合性能較好，佔用顯存較少，訓練用時較少。
RWKV 整體結構依然採用 Transformer Block 的思路，相較於原始 Transformer Block 的結構，RWKV 將 self-attention 替換為 Position Encoding 和 TimeMix，將 FFN 替換為 ChannelMix。其餘部分與 Transfomer 一致。
白澤 —— 使用 LoRA 訓練的大語言模型
白澤是使用 LoRA 訓練的開源聊天模型，它改進了開源大型語言模型 LLaMA，通過使用新生成的聊天語料庫對 LLaMA 進行微調，該模型在單個 GPU 上運行，使其可供更廣泛的研究人員使用。
白澤目前包括四種英語模型：白澤 -7B、13B 和 30B（通用對話模型），以及一個垂直領域的白澤 - 醫療模型，供研究 / 非商業用途使用，並計畫在未來發佈中文的白澤模型。
白澤的資料處理、訓練模型、Demo 等全部代碼已經開源。
CodeGeeX —— 多語言代碼生成模型
CodeGeeX 是一個具有 130 億參數的多程式設計語言代碼生成預訓練模型。CodeGeeX 採用華為 MindSpore 框架實現，在鵬城實驗室 “鵬城雲腦 II” 中的 192 個節點（共 1536 個國產昇騰 910 AI 處理器）上訓練而成。
CodeGeeX 有以下特點：
•	高精度代碼生成：支援生成 Python、C++、Java、JavaScript 和 Go 等多種主流程式設計語言的代碼，在 HumanEval-X 代碼生成任務上取得 47%~60% 求解率，較其他開源基線模型有更佳的平均性能。
•	跨語言代碼翻譯：支援代碼片段在不同程式設計語言間進行自動翻譯轉換，翻譯結果正確率高，在 HumanEval-X 代碼翻譯任務上超越了其它基線模型。
•	自動程式設計外掛程式：CodeGeeX 外掛程式現已上架 VSCode 外掛程式市場（完全免費），用戶可以通過其強大的少樣本生成能力，自訂代碼生成風格和能力，更好輔助代碼編寫。
•	模型跨平臺開源: 所有代碼和模型權重開源開放，用作研究用途。CodeGeeX 同時支持昇騰和英偉達平臺，可在單張昇騰 910 或英偉達 V100/A100 上實現推理。
Falcon LLM —— 開來源語言模型
「Falcon」由阿聯酋阿布達比的技術創新研究所（TII）開發，從性能上看，Falcon比LLaMA的表現更好。TII表示，Falcon迄今為止最強大的開來源語言模型。其最大的版本，Falcon 40B，擁有400億參數，相對於擁有650億參數的LLaMA來說，規模上還是小了一點。規模雖小，性能能打。
Vicuna —— 基於 LLaMA 的微調大語言模型
Vicuna 模型對 LLaMA 進行了微調，由加州大學伯克利分校、卡內基梅隆大學、斯坦福大學、加州大學聖地牙哥分校和 MBZUAI 的學術團隊進行微調訓練而成，有兩種大小可供選擇：7B 和 13B。
Vicuna-13B 與 Stanford Alpaca 等其他開源模型相比展示了具有競爭力的性能。
以 GPT-4 為評判標準的初步評估顯示，Vicuna-13B 達到了 OpenAI ChatGPT 和 Google Bard 90% 以上的品質，同時在 90% 以上的情況下超過了 LLaMA 和 Stanford Alpaca 等其他模型的表現。訓練 Vicuna-13B 成本約為 300 美元。訓練和服務代碼，以及線上演示都是公開的，可用於非商業用途。
RedPajama —— 1.2 萬億資料集的可商用大語言模型
RedPajama 專案旨在創建一套領先的全開源大語言模型。目前，該項目已完成了第一步，成功複製了 LLaMA 訓練資料集超過 1.2 萬億個資料 token。該項目由 Together、Ontocord.ai、ETH DS3Lab、斯坦福大學 CRFM、Hazy Research 和 MILA 魁北克 AI 研究所聯合開發。
RedPajama 包含三個主要組成部分：預訓練資料、基礎模型和指令調優資料與模型。
OpenAssistant —— 基於對話的大型語言模型
OpenAssistant 是一個開源專案，旨在開發免費提供給所有人使用的 AI 聊天機器人。
訓練資料集 OpenAssistant Conversations 包含了超過 60 萬個涉及各種主題的交互，用於訓練各種模型。目前發佈了經過指令調整的 LLaMA 13B 和 30B 模型，以及其他使用相同資料集訓練的模型。
StableLM —— Stability AI 開發的語言模型
StableLM 專案倉庫包含 Stability AI 正在進行的 StableLM 系列語言模型開發，目前 Stability AI 發佈了初始的 StableLM-alpha 模型集，具有 30 億和 70 億參數。150 億和 300 億參數的模型正在開發中。
StableLM 模型可以生成文本和代碼，並為一系列下游應用提供支援。它們展示了小而高效的模型如何在適當的訓練下提供高性能。
StarCoder —— AI 程式設計模型
StarCoder（150 億參數）是 Hugging Face 聯合 ServiceNow 發佈的免費大型語言模型，該模型經過訓練主要用途是可以生成代碼，目的是為了對抗 GitHub Copilot 和亞馬遜 CodeWhisperer 等基於 AI 的程式設計工具。
SantaCoder —— 羽量級 AI 程式設計模型
SantaCoder 是一個語言模型，該模型擁有 11 億個參數，可以用於 Python、Java 和 JavaScript 這幾種程式設計語言的代碼生成和補全建議。
根據官方提供的資訊，訓練 SantaCoder 的基礎是 The Stack（v1.1）資料集，SantaCoder 雖然規模相對較小，只有 11 億個參數，在參數的絕對數量上低於 InCoder（67 億）或 CodeGen-multi（27 億），但 SantaCoder 的表現則是要遠好於這些大型多語言模型。
MLC LLM —— 本地大語言模型
MLC LLM 是一種通用解決方案，它允許將任何語言模型本地部署在各種硬體後端和本地應用程式上。
此外，MLC LLM 還提供了一個高效的框架，供使用者根據需求進一步優化模型性能。MLC LLM 旨在讓每個人都能在個人設備上本地開發、優化和部署 AI 模型，而無需伺服器支援，並通過手機和筆記型電腦上的消費級 GPU 進行加速。
Web LLM —— 流覽器大語言模型
Web LLM 是一個可將大型語言模型和基於 LLM 的聊天機器人引入 Web 流覽器的項目。一切都在流覽器內運行，無需伺服器支援，並使用 WebGPU 加速。這開闢了許多有趣的機會，可以為每個人構建 AI 助手，並在享受 GPU 加速的同時實現隱私。
WizardLM —— 基於 LLaMA 的微調大語言模型
WizardLM 是一個經過微調的 7B LLaMA 模型。它通過大量具有不同難度的指令跟隨對話進行微調。這個模型的新穎之處在於使用了 LLM 來自動生成訓練資料。
WizardLM 模型使用一種名為 Evol-Instruct（是一種使用 LLM 代人類自主批生成各種難度等級和技術範圍的開放指令，以提高 LLM 能力的新方法）的新方法，通過 70k 個電腦生成的指令進行訓練，該方法生成具有不同難度級別的指令。
YaLM 100B —— 千億參數預訓練語言模型
YaLM 100B是一個類似 GPT 的神經網路，用於生成和處理文本。
該模型利用了 1000 億個參數，在 800 個 A100 顯卡和 1.7 TB 線上文本、書籍以及海量其他英文和俄文資源的集群上訓練該模型花了 65 天時間。
OpenLLaMA —— LLaMA 大語言模型的開源複現版本
OpenLLaMA 是 Meta AI 的 LLaMA 大語言模型的開源複現版本，採用寬鬆許可證。
倉庫包含經過訓練的 2000 億標記的 7B OpenLLaMA 模型的公共預覽版，並提供了預訓練的 OpenLLaMA 模型的 PyTorch 和 Jax 權重，以及評估結果和與原始 LLaMA 模型的比較。
LLM 相關工具
OpenLLM —— 操作大語言模型的開放平臺
OpenLLM 是一個生產級的操作大語言模型 (LLM) 的開放平臺。支援便捷 Fine-tune 微調、Serve 模型服務、部署和監控任何 LLM。借助 OpenLLM，可以使用任何開源大語言模型運行推理，部署到雲端或本地，並構建強大的 AI 應用程式。
OpenLLM 特性包括：
•	先進的 LLM：內置支援各種開源 LLM 和模型運行時，包括 StableLM、Falcon、Dolly、Flan-T5、ChatGLM、StarCoder 等。
•	靈活的 API：只需一個命即可通過 RESTful API 或 gRPC 為 LLM 提供服務，並通過 WebUI、CLI、Python/Javascript 用戶端或任何 HTTP 用戶端進行查詢。
•	自由構建：對 LangChain、BentoML 和 Hugging Face 具有一級支援，可以通過將 LLM 與其它模型和服務組合來輕鬆創建自己的 AI 應用程式。
•	簡化部署：自動生成 LLM 伺服器 Docker 鏡像或通過 BentoCloud 部署為無伺服器端節點。
•	自建 LLM：使用 LLM.tuning() 微調任何 LLM 以滿足特定需求。 （即將推出）
LangChain —— 構建 LLM 應用的工具
LangChain 是一個用於構建基於大型語言模型（LLM）的應用程式的庫。它可以幫助開發者將 LLM 與其他計算或知識源結合起來，創建更強大的應用程式。
LangChain 提供了以下幾個主要模組來支援這些應用程式的開發：
•	Prompts：這包括提示管理、提示優化和提示序列化。
•	LLMs：這包括所有 LLM 的通用介面，以及與 LLM 相關的常用工具。
•	Document Loaders：這包括載入文檔的標準介面，以及與各種文本資料來源的特定集成。
•	Utils：語言模型在與其他知識或計算源交互時通常更強大。這可能包括 Python REPL、嵌入、搜尋引擎等。LangChain 提供了一系列常用的工具來在應用程式中使用。
•	Chains：Chains 不僅僅是一個單獨的 LLM 調用，而是一系列的調用（無論是對 LLM 還是其他工具）。LangChain 提供了鏈的標準介面，許多與其他工具的集成，以及常見應用程式的端到端鏈。
•	Indexes：語言模型在與自己的文本資料結合時通常更強大 - 這個模組涵蓋了這樣做的最佳實踐。
•	Agents：Agents 涉及到一個 LLM 在決定採取哪些行動、執行該行動、看到一個觀察結果，並重複這個過程直到完成。LangChain 提供了代理的標準介面，可供選擇的代理，以及端到端代理的示例。
•	Memory：Memory 是在鏈 / 代理調用之間持久化狀態的概念。LangChain 提供了記憶體的標準介面，一系列記憶體實現，以及使用記憶體的鏈 / 代理示例。
•	Chat：Chat 模型是一種與語言模型不同的 API - 它們不是使用原始文本，而是使用消息。LangChain 提供了一個標準介面來使用它們，並做所有上述相同的事情。
JARVIS —— 連接 LLM 和 AI 模型的協作系統
JARVIS 是用於連接 LLM 和 AI 模型的協作系統。該系統由 LLM（大語言模型）作為控制器和許多AI 模型作為協作執行者（來自 HuggingFace Hub）組成。
系統的工作流程包括四個階段：
•	任務規劃：使用 ChatGPT 分析用戶的請求，瞭解他們的意圖，並將其拆解成可解決的任務。
•	模型選擇：為了解決計畫的任務，ChatGPT 根據描述選擇託管在 Hugging Face 上的 AI 模型。
•	任務執行：調用並執行每個選定的模型，並將結果返回給 ChatGPT。
•	生成回應: 最後使用 ChatGPT 整合所有模型的預測，生成 Response。
Semantic Kernel —— 集成 LLM 到應用程式的 SDK
Semantic Kernel 是一種羽量級 SDK，可將 AI 大語言模型 (LLM) 與傳統程式設計語言集成。
Semantic Kernel 可擴展程式設計模型結合了自然語言語義功能、傳統代碼原生功能和基於嵌入的記憶體，釋放新的潛力並通過 AI 為應用程式增加價值。
 
Semantic Kernel 旨在支援和封裝來自最新 AI 研究的多種設計模式，以便開發人員可以為他們的應用程式注入複雜的技能，如提示鏈、遞迴推理、總結、零 / 少樣本學習、上下文記憶、長期記憶、嵌入、語義索引、規劃和訪問外部知識存儲以及內部資料等功能。
LMFlow —— 大語言模型的可擴展工具包
LMFlow 由香港科技大學統計和機器學習實驗室團隊發起，致力於建立一個全開放的大模型研究平臺，支援有限機器資源下的各類實驗，並且在平臺上提升現有的資料利用方式和優化演算法效率，讓平臺發展成一個比之前方法更高效的大模型訓練系統。
LMFlow 的最終目的是説明每個人都可以用儘量少的資源來訓練一個專有領域的、個性化的大模型，以此來推進大模型的研究和應用落地。
LMFlow 擁有四大特性：可擴展、羽量級、定制化和完全開源。
 
基於此，使用者可以很快地訓練自己的模型並繼續進行二次反覆運算。這些模型不僅限於最近流行的 LLaMA，也包括 GPT-2、Galactica 等模型。
xturing —— LLM 個性化微調工具
xturing 為 LLM 提供了快速、高效和簡單的微調，如 LLaMA、GPT-J、GPT-2、OPT、Cerebras-GPT、Galactica 等。通過提供一個易於使用的介面，再根據你自己的資料和應用來個性化 LLM，xTuring 使構建和控制 LLM 變得簡單。整個過程可以在你的電腦內或在你的私有雲中完成，確保資料的隱私和安全。
通過 xturing，你可以：
•	從不同的來源攝取資料，並將其預處理成 LLM 可以理解的格式
•	從單個 GPU 擴展到多個 GPU，以便更快地進行微調
•	利用記憶體效率高的技術（即 LoRA 微調）來減少你的硬體成本，最多可減少 90% 的時間。
•	探索不同的微調方法，並以它們為基準，找到性能最好的模型
•	在明確定義的指標上評估微調模型，進行深入分析
Dify —— 易用的 LLMOps 平臺
Dify是一個易用的 LLMOps 平臺，旨在讓更多人可以創建可持續運營的原生 AI 應用。Dify 提供多種類型應用的視覺化編排，應用可開箱即用，也能以 “後端即服務” 的 API 提供服務。
“Dify” 這個名字來源於 “Define” 和 “Modify” 這兩個詞。它代表了幫助開發人員不斷改進其 AI 應用程式的願景。“Dify” 可以理解為 “Do it for you”。
通過 Dify 創建的應用包含了：
•	開箱即用的的 Web 網站，支援表單模式和聊天對話模式
•	一套 API 即可包含外掛程式、上下文增強等能力，替你省下了後端代碼的編寫工作
•	視覺化的對應用進行資料分析，查閱日誌或進行標注
Dify 相容 Langchain，這意味著將逐步支持多種 LLMs ，目前已支持：
•	GPT 3 (text-davinci-003)
•	GPT 3.5 Turbo(ChatGPT)
•	GPT-4
Dify.AI 核心能力
•	視覺化編排 Prompt：通過介面化編寫 prompt 並調試，只需幾分鐘即可發佈一個 AI 應用。
•	接入長上下文（資料集）：全自動完成文本預處理，使用你的資料作為上下文，無需理解晦澀的概念和技術處理。
•	基於 API 開發後端即服務。你可以直接訪問網頁應用，也可以接入 API 集成到你的應用中，無需關注複雜的後端架構和部署過程。
•	資料標注與改進：視覺化查閱 AI 日誌並對資料進行改進標注，觀測 AI 的推理過程，不斷提高其性能。
正在開發中的功能：
•	資料集，支持更多的資料集，例如同步 Notion 或網頁的內容。將支援更多的資料集，包括文本、網頁，甚至 Notion 內容。使用者可以根據自己的資料來源構建 AI 應用程式。
•	外掛程式，推出符合 ChatGPT 標準的外掛程式，或使用 Dify 產生的外掛程式。將發佈符合 ChatGPT 標準的外掛程式，或者 Dify 自己的外掛程式，以在應用程式中啟用更多功能。
•	開源模型，例如採用 Llama 作為模型提供者，或進行進一步的微調 。將與優秀的開源模型如 Llama 合作，通過在平臺中提供它們作為模型選項，或使用它們進行進一步的微調。
Flowise —— 輕鬆構建 LLM 應用程式
Flowise 是一個開源 UI 視覺化工具，使用以 Node Typescript/Javascript 編寫的 LangchainJS 構建自訂 LLM 流程。
•	LLM Chain：帶有提示範本和 LLM 模型的 LLM Chain的基本示例
 
•	Language Translation Chain：使用帶有聊天提示範本和聊天模型的 LLM Chain 進行語言翻譯
 
•	有記憶的會話代理：聊天模型的會話代理，它利用聊天特定提示和緩衝記憶體
 
Jigsaw Datase —— 提高大型語言模型性能的工具
Jigsaw 是微軟推出的一種可以提高大型語言模型性能（如 GPT-3、Codex 等）的新工具。
Jigsaw 部署了理解程式語法和語義的後處理技術，然後利用用戶回饋來提高未來的性能；該工具旨在使用多模式輸入為 Python Pandas API 合成代碼。Pandas 是資料科學中廣泛使用的 API，具有數百個用於 manipulating dataframes 或具有行和列的表的函數。
目標是使部分審查自動化，以提高使用 Codex 等大型語言模型進行代碼合成的開發人員的生產力。
Jigsaw 獲取英語查詢並使用適當的上下文對其進行預處理，以構建可以饋送到大型語言模型的輸入。該模型被視為一個黑盒子，並且 Jigsaw 已使用 GPT-3 和 Codex 進行了評估。這種設計的優勢在於它支持隨插即用最新和最好的可用型號。
微軟在實驗中發現，Jigsaw 可以在 30% 的時間內創建正確的輸出。如果代碼失敗，那麼修復過程在後處理階段開始。
 
GPTCache —— 為 LLM 查詢創建語義緩存的庫
GPTCache 是一個用於創建語義緩存以存儲來自 LLM 查詢的回應的庫。將你的 LLM API 成本削減 10 倍，將速度提高 100 倍。
ChatGPT 和各種大型語言模型（LLM）擁有令人難以置信的多功能性，能夠開發廣泛的應用程式。然而，隨著你的應用程式越來越受歡迎，遇到更高的流量水準，與 LLM API 調用相關的費用可能會變得很高。此外，LLM 服務可能會表現出緩慢的回應時間，特別是在處理大量的請求時。GPTCache 的創建就是為了應對這一挑戰，這是一個致力於建立一個用於存儲 LLM 響應的語義緩存的項目。
聞達 —— LLM 調用平臺
聞達：一個大型語言模型調用平臺。目前支持 chatGLM-6B、chatRWKV、chatYuan 和 chatGLM-6B 模型下自建知識庫查找。
1.	目前支援模型：chatGLM-6B、chatRWKV、chatYuan。
2.	知識庫自動查找
3.	支持參數線上調整
4.	支援chatGLM-6B、chatRWKV流式輸出和輸出過程中中斷
5.	自動保存對話歷史至流覽器（多用戶同時使用不會衝突）
6.	對話歷史管理（刪除單條、清空）
7.	支援局域網、內網部署和多用戶同時使用。（內網部署需手動將前段靜態資源切換成本地）
8.	多使用者同時使用中會自動排隊，並顯示當前使用者。
設置和預設功能
 
預設功能使用
 
MindFormers ——大模型訓練/推理/部署全流程開發套件
MindSpore MindFormers 套件的目標是構建一個大模型訓練、推理、部署的全流程開發套件： 提供業內主流的 Transformer 類預訓練模型和 SOTA 下游任務應用，涵蓋豐富的並行特性。 期望説明使用者輕鬆的實現大模型訓練和創新研發。
MindSpore MindFormers 套件基於 MindSpore 內置的並行技術和組件化設計，具備如下特點：
•	一行代碼實現從單卡到大規模集群訓練的無縫切換。
•	提供靈活易用的個性化並行配置。
•	能夠自動進行拓撲感知，高效地融合資料並行和模型並行策略。
•	一鍵啟動任意任務的訓練、評估、推理流程。
•	支援使用者進行元件化配置任意模組，如優化器、學習策略、網路組裝等。
•	提供 Trainer、ModelClass、ConfigClass、pipeline 等高階易用性介面。
目前支援的模型清單如下：
•	BERT
•	GPT
•	OPT
•	T5
•	MAE
•	SimMIM
•	CLIP
•	FILIP
•	Vit
•	Swin
Code as Policies —— 自然語言代碼生成系統
Code as Policies 是一種以機器人為中心的語言模型生成的程式在物理系統上執行的表述。CaP 擴展了 PaLM-SayCan，使語言模型能夠通過通用 Python 代碼的完整表達來完成更複雜的機器人任務。通過 CaP，Google 建議使用語言模型，通過少量的提示來直接編寫機器人代碼。實驗證明，與直接學習機器人任務和輸出自然語言動作相比，CaP 輸出代碼表現更好。CaP 允許單一系統執行各種複雜多樣的機器人任務，而不需要特定的任務訓練。
 
用於控制機器人的常見方法是用代碼對其進行程式設計，以檢測物體、移動執行器的排序命令和反饋回路來指定機器人應如何執行任務。但為每項新任務重新程式設計的可能很耗時，而且需要領域的專業知識。
Colossal-AI —— 大模型並行訓練系統
ColossalAI 是一個具有高效並行化技術的綜合大規模模型訓練系統。旨在無縫整合不同的並行化技術範式，包括資料並行、管道並行、多張量並行和序列並行。
Colossal-AI 的目標是支援人工智慧社區以與他們正常編寫模型相同的方式編寫分散式模型。這使得他們可以專注于開發模型架構，並將分散式訓練的問題從開發過程中分離出來。
ColossalAI 提供了一組並行訓練元件。旨在支援使用者編寫分散式深度學習模型，就像編寫單 GPU 模型一樣。提供友好的工具，只需幾行即可啟動分散式培訓。
BentoML 統一模型部署框架
BentoML 是 AI 應用程式開發人員的平臺，提供工具和基礎架構來簡化整個 AI 產品開發生命週期。BentoML 使創建準備好部署和擴展的機器學習服務變得容易。
BentoML 原生支持所有流行的 ML 框架，包括 Pytorch、Tensorflow、JAX、XGBoost、HuggingFace、MLFlow，以及最新的預構建開源 LLM（大型語言模型）和生成式 AI 模型。
NSQL —— 開源 SQL 協同生成基礎模型
NSQL，這是一個專為 SQL 生成任務設計的全新開源大型基礎模型 （FM） 系列，包括 NSQL 350M、NSQL 2B 和 NSQL 6B。
 
Highlights
Unified Model Serving API
•	適用於 Tensorflow、PyTorch、XGBoost、Scikit-Learn、ONNX 等的框架無關的模型打包
•	為預處理 / 後處理和業務邏輯編寫自訂 Python 代碼以及模型推理
•	為線上（REST API 或 gRPC）、離線批次處理和流式推理應用相同的代碼
•	用於構建多模型推理管道或圖形的簡單抽象
無摩擦過渡到生產的標準化流程
•	將 Bento 構建為 ML 服務的標準可部署工件
•	自動生成具有所需依賴項的 docker 鏡像
•	使用 GPU 進行推理的簡單 CUDA 設置
•	與 MLOps 生態系統的豐富集成，包括 Kubeflow、Airflow、MLFlow、Triton
具有強大的性能優化的可擴展性
•	自我調整批次處理根據伺服器端最佳性能動態分組推理請求
•	Runner 抽象將模型推理與你的自訂代碼分開進行 scales
•	通過自動配置最大化你的 GPU 和多核 CPU 利用率
以 DevOps 友好的方式部署到任何地方
•	通過以下方式簡化生產部署工作流程：
o	BentoML Cloud：部署便當的最快方式，簡單且大規模
o	Yatai：在 Kubernetes 上大規模部署模型
o	bentoctl：在 AWS SageMaker、Lambda、ECE、GCP、Azure、Heroku 等平臺上快速部署模型！
•	使用 Spark 或 Dask 運行離線批量推理作業
•	對 Prometheus 指標和 OpenTelemetry 的內置支持
•	用於高級 CI/CD 工作流程的靈活 API
